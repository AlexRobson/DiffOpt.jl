var documenterSearchIndex = {"docs":
[{"location":"intro/#Introduction","page":"Introduction","title":"Introduction","text":"","category":"section"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"An optimization problem is the problem of finding the best solution from all feasible solutions. The standard form of an optimization problem is ","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"beginaligned\nunderset xoperatorname minimize f(x)operatorname subjectto g_i(x)leq 0quad i=1dots mh_j(x)=0quad j=1dots p\nendaligned","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"Note that finding solution to most of the optimization problems is computationally intractable. Here we consider a subset of those problems called convex optimization problems, which admit polynomial time solutions. The standard form of a convex optimization problem is ","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"beginaligned\nunderset xoperatorname minimize f(x)operatorname subjectto g_i(x)leq 0quad i=1dots mA x = b\nendaligned","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"where f and g_i are convex functions.","category":"page"},{"location":"intro/#Parameterized-problems","page":"Introduction","title":"Parameterized  problems","text":"","category":"section"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"In practice, convex optimization problems include parameters, apart from the decision variables, which determines the structure of the problem itself i.e. the objective function and constraints. Hence they affect the solution too. A general form of a parameterized convex optimization problem is ","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"beginaligned\nunderset xoperatorname minimize f(x theta)operatorname subjectto g_i(x theta)leq 0quad i=1dots mA(theta) x = b(theta)\nendaligned","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"where theta is the parameter. In different fields, these parameters go by different names:","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"Hyperparameters in machine learning\nRisk aversion or other backtesing parameters in financial modelling\nParameterized systems in control theory","category":"page"},{"location":"intro/#What-do-we-mean-by-differentiating-a-parameterized-optimization-program?-Why-do-we-need-it?","page":"Introduction","title":"What do we mean by differentiating a parameterized optimization program? Why do we need it?","text":"","category":"section"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"Often, parameters are chosen and tuned by hand - an iterative process - and the structure of the problem is crafted manually. But it is possible to do an automatic gradient based tuning of parameters.","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"Consider solution of the parametrized optimization problem, x(theta),","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"beginsplit\nbeginarray lll\nx^*(theta)= underset xoperatorname argmin  f(x theta)\n              operatorname subjectto  g_i(x theta)leq 0quad i=1dots m\n                                           A(theta) x = b(theta)\nendarray\nendsplit","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"which is the input of l(x^*(theta)), a loss function. Our goal is to choose the best parameter theta so that l is optimized. Here, l(x^*(theta)) is the objective function and theta is the decision variable. In order to apply a gradient-based strategy to this problem, we need to differentiate l with respect to theta.","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"fracpartial l(x^*(theta))partial theta = fracpartial l(x^*(theta))partial x^*(theta)  fracpartial x^*(theta)partial theta","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"By implicit function theorem, this translates to differentiating the program data, i.e. functions f, g_i(x) and matrices A, b, with respect to theta.","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"This is can be achieved in two steps or passes:","category":"page"},{"location":"intro/","page":"Introduction","title":"Introduction","text":"Forward pass - Given an initial value of theta, solves the optimization problem to find x^*(theta)\nBackward pass - Given x^*, differentiate and find fracpartial x^*(theta)partial theta","category":"page"},{"location":"solve-conic-1/#Solving-conic-with-PSD-and-SOC-constraints","page":"Solving conic with PSD and SOC constraints","title":"Solving conic with PSD and SOC constraints","text":"","category":"section"},{"location":"solve-conic-1/","page":"Solving conic with PSD and SOC constraints","title":"Solving conic with PSD and SOC constraints","text":"Consider an example program ","category":"page"},{"location":"solve-conic-1/","page":"Solving conic with PSD and SOC constraints","title":"Solving conic with PSD and SOC constraints","text":"beginsplit\nbeginarray llcc\nmboxminimize  \nleftlangle\nleft\nbeginarray ccc\n        2  1  0  \n        1  2  1  \n        0  1  2\n   endarray\n   right\n        X rightrangle\n        + x_0        \n          mboxsubject to  \n          leftlangle\n          left\n          beginarray ccc\n          1  0  0  \n          0  1  0  \n          0  0  1\n          endarray\n          right\n          X rightrangle\n          + x_0  =   1   \n            \n            leftlangle\n            left\n            beginarrayccc\n            1  1  1  \n            1  1  1  \n            1  1  1\n            endarray\n            right\n            X rightrangle + x_1 + x_2\n             =  12  \n             (x_0 x_1 x_2) in mathbbQ^3 text or  x_0 geq sqrtx_1^2 + x_2^2 \n             X succeq 0 X in mathbbS^3_+\nendarray\nendsplit","category":"page"},{"location":"solve-conic-1/","page":"Solving conic with PSD and SOC constraints","title":"Solving conic with PSD and SOC constraints","text":"where","category":"page"},{"location":"solve-conic-1/","page":"Solving conic with PSD and SOC constraints","title":"Solving conic with PSD and SOC constraints","text":"mathbbS^n_+ =\nleftlbrace\nX in mathbbS^n z^T X z geq 0 quad forall z in mathbbR^n\nrightrbrace","category":"page"},{"location":"solve-conic-1/","page":"Solving conic with PSD and SOC constraints","title":"Solving conic with PSD and SOC constraints","text":"Refered from Mosek examples: https://docs.mosek.com/9.2/toolbox/tutorial-sdo-shared.html#example-sdo1","category":"page"},{"location":"solve-conic-1/#Equivalent-DiffCP-program-to-differentiate","page":"Solving conic with PSD and SOC constraints","title":"Equivalent DiffCP program to differentiate","text":"","category":"section"},{"location":"solve-conic-1/","page":"Solving conic with PSD and SOC constraints","title":"Solving conic with PSD and SOC constraints","text":"import numpy as np\nimport cvxpy as cp\nfrom scipy import sparse\nimport diffcp\n\nA = sparse.csc_matrix((11+1,7+1), dtype=np.float64)\nA[2 , 1]  =  1.0\nA[3 , 1]  =  -1.0\nA[9 , 1]  =  -0.45\nA[10, 1]  =  0.45\nA[11, 1]  =  -0.45\nA[2 , 2]  =  1.0\nA[4 , 2]  =  -1.0\nA[9 , 2]  =  -0.8\nA[10, 2]  =  0.318198\nA[11, 2]  =  -0.1\nA[2 , 3]  =  1.0\nA[5 , 3]  =  -1.0\nA[9 , 3]  =  -0.9\nA[2 , 4]  =  1.0\nA[6 , 4]  =  -1.0\nA[9 , 4]  =  -0.225\nA[2 , 5]  =  1.0\nA[7 , 5]  =  -1.0\nA[9 , 5]  =  -0.1125\nA[10, 5]  =  0.1125\nA[11, 5]  =  -0.1125\nA[2 , 6]  =  1.0\nA[8 , 6]  =  -1.0\nA[11, 6]  =  -0.225\nA[9 , 7]  =  1.0\nA[11, 7]  =  1.0\n\nA = A[1:, 1:]\n\n# equivalent to: https://github.com/jump-dev/MathOptInterface.jl/blob/master/src/Test/contconic.jl#L2575\n\ncone_dict = {\n    diffcp.POS: 7,\n    diffcp.PSD: [2],\n    diffcp.ZERO: 1\n}\n\nb = np.array([0.0, 10.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, 0.0, 0.0, 0.0])\nc = np.array([-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -1.0])\n\nx, y, s, D, DT = diffcp.solve_and_derivative(A, b, c, cone_dict)\nprint(x) # MOI.VariablePrimal\nprint(s) # MOI.ConstraintPrimal\nprint(y) # MOI.ConstraintDual\n\n\ndx, dy, ds = D(sparse.csc_matrix(np.ones((11,7))), np.ones(11), np.ones(7))\nprint(dx)\nprint(ds)\nprint(dy)","category":"page"},{"location":"solve-conic-1/#Equivalent-DiffOpt-program","page":"Solving conic with PSD and SOC constraints","title":"Equivalent DiffOpt program","text":"","category":"section"},{"location":"solve-conic-1/","page":"Solving conic with PSD and SOC constraints","title":"Solving conic with PSD and SOC constraints","text":"using SCS\nusing DiffOpt\nusing MathOptInterface\n\nconst MOI = MathOptInterface;\n\n\nmodel = diff_optimizer(SCS.Optimizer)\nMOI.set(model, MathOptInterface.Silent(), true)\n\nδ = √(1 + (3*√2+2)*√(-116*√2+166) / 14) / 2\nε = √((1 - 2*(√2-1)*δ^2) / (2-√2))\ny2 = 1 - ε*δ\ny1 = 1 - √2*y2\nobj = y1 + y2/2\nk = -2*δ/ε\nx2 = ((3-2obj)*(2+k^2)-4) / (4*(2+k^2)-4*√2)\nα = √(3-2obj-4x2)/2\nβ = k*α\n\nX = MOI.add_variables(model, 6)\nx = MOI.add_variables(model, 3)\n\nvov = MOI.VectorOfVariables(X)\n\ncX = MOI.add_constraint(\n    model, \n    MOI.VectorAffineFunction{Float64}(vov), MOI.PositiveSemidefiniteConeTriangle(3)\n)\n\ncx = MOI.add_constraint(\n    model, \n    MOI.VectorAffineFunction{Float64}(MOI.VectorOfVariables(x)), MOI.SecondOrderCone(3)\n)\n\nc1 = MOI.add_constraint(\n    model, \n    MOI.VectorAffineFunction(\n        MOI.VectorAffineTerm.(1:1,\n            MOI.ScalarAffineTerm.([1., 1., 1., 1.], [X[1], X[3], X[end], x[1]])), \n        [-1.0]\n    ), \n    MOI.Zeros(1)\n)\n\nc2 = MOI.add_constraint(\n    model, \n    MOI.VectorAffineFunction(\n        MOI.VectorAffineTerm.(1:1,\n            MOI.ScalarAffineTerm.([1., 2, 1, 2, 2, 1, 1, 1], [X; x[2]; x[3]])), \n        [-0.5]\n    ), \n    MOI.Zeros(1)\n)\n\nobjXidx = [1:3; 5:6]\nobjXcoefs = 2*ones(5)\nMOI.set(model, MOI.ObjectiveFunction{MOI.ScalarAffineFunction{Float64}}(),\n    MOI.ScalarAffineFunction(\n        MOI.ScalarAffineTerm.([objXcoefs; 1.0], [X[objXidx]; x[1]]), 0.0))\nMOI.set(model, MOI.ObjectiveSense(), MOI.MIN_SENSE)\n\nsol = MOI.optimize!(model)\n\n# fetch solution\nx_sol = MOI.get(model, MOI.VariablePrimal(), vcat(X, x))\ns_sol = MOI.get(model, MOI.ConstraintPrimal(), [cX, cx, c1, c2])\ny_sol = MOI.get(model, MOI.ConstraintDual(), [cX, cx, c1, c2])\n\nprintln(\"x -> \", round.(x_sol; digits=3))\nprintln(\"s -> \", round.(s_sol; digits=3))\nprintln(\"y -> \", round.(y_sol; digits=3))\n\n# perturbations in all the parameters\nfx = MOI.SingleVariable.(x)\nMOI.set(model,\n    DiffOpt.ForwardInConstraint(), c1, MOI.Utilities.vectorize(ones(1, 9) * fx + ones(1)))\nMOI.set(model,\n    DiffOpt.ForwardInConstraint(), c2, MOI.Utilities.vectorize(ones(6, 9) * fx + ones(6)))\nMOI.set(model,\n    DiffOpt.ForwardInConstraint(), c3, MOI.Utilities.vectorize(ones(3, 9) * fx + ones(3)))\nMOI.set(model,\n    DiffOpt.ForwardInConstraint(), c4, MOI.Utilities.vectorize(ones(1, 9) * fx + ones(1)))\n\n# differentiate and get the gradients\nDiffOpt.forward(model)\n\ndx = MOI.get.(model,\n    DiffOpt.ForwardOutVariablePrimal(), vcat(X, x))\n\nprintln(\"dx -> \", round.(dx; digits=3))\n# println(\"ds -> \", round.(ds; digits=3))\n# println(\"dy -> \", round.(dy; digits=3))","category":"page"},{"location":"manual/#Manual","page":"Manual","title":"Manual","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"note: Note\nAs of now, this package only works for optimization models that can be written either in convex conic form or convex quadratic form.","category":"page"},{"location":"manual/#Supported-objectives-and-constraints-scheme-1","page":"Manual","title":"Supported objectives & constraints - scheme 1","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"For QPTH/OPTNET style backend, the package supports following Function-in-Set constraints: ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"MOI Function MOI Set\nSingleVariable GreaterThan\nSingleVariable LessThan\nSingleVariable EqualTo\nScalarAffineFunction GreaterThan\nScalarAffineFunction LessThan\nScalarAffineFunction EqualTo","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"and the following objective types: ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"MOI Function\nSingleVariable\nScalarAffineFunction\nScalarQuadraticFunction","category":"page"},{"location":"manual/#Supported-objectives-and-constraints-scheme-2","page":"Manual","title":"Supported objectives & constraints - scheme 2","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"For DiffCP/CVXPY style backend, the package supports following Function-in-Set constraints: ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"MOI Function MOI Set\nVectorOfVariables Nonnegatives\nVectorOfVariables Nonpositives\nVectorOfVariables Zeros\nVectorOfVariables SecondOrderCone\nVectorOfVariables PositiveSemidefiniteConeTriangle\nVectorAffineFunction Nonnegatives\nVectorAffineFunction Nonpositives\nVectorAffineFunction Zeros\nVectorAffineFunction SecondOrderCone\nVectorAffineFunction PositiveSemidefiniteConeTriangle","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"and the following objective types: ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"MOI Function\nSingleVariable\nScalarAffineFunction","category":"page"},{"location":"manual/#Creating-a-differentiable-optimizer","page":"Manual","title":"Creating a differentiable optimizer","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"You can create a differentiable optimizer over an existing MOI solver by using the diff_optimizer utility. ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"diff_optimizer","category":"page"},{"location":"manual/#DiffOpt.diff_optimizer","page":"Manual","title":"DiffOpt.diff_optimizer","text":"diff_optimizer(optimizer_constructor)::Optimizer\n\nCreates a DiffOpt.Optimizer, which is an MOI layer with an internal optimizer and other utility methods. Results (primal, dual and slack values) are obtained by querying the internal optimizer instantiated using the optimizer_constructor. These values are required for find jacobians with respect to problem data.\n\nOne define a differentiable model by using any solver of choice. Example:\n\njulia> using DiffOpt, GLPK\n\njulia> model = diff_optimizer(GLPK.Optimizer)\njulia> model.add_variable(x)\njulia> model.add_constraint(...)\n\njulia> _backward_quad(model)  # for convex quadratic models\n\njulia> _backward_quad(model)  # for convex conic models\n\n\n\n\n\n","category":"function"},{"location":"manual/#Adding-new-sets-and-constraints","page":"Manual","title":"Adding new sets and constraints","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"The DiffOpt Optimizer behaves similarly to other MOI Optimizers and implements the MOI.AbstractOptimizer API.","category":"page"},{"location":"manual/#Projections-on-cone-sets","page":"Manual","title":"Projections on cone sets","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"DiffOpt requires taking projections and finding projection gradients of vectors while computing the jacobians. For this purpose, we use MathOptSetDistances.jl, which is a dedicated package for computing set distances, projections and projection gradients.","category":"page"},{"location":"manual/#Conic-problem-formulation","page":"Manual","title":"Conic problem formulation","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"note: Note\nAs of now, the package is using SCS geometric form for affine expressions in cones.","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"Consider a convex conic optimization problem in its primal (P) and dual (D) forms:","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"beginsplit\nbeginarray llcc\ntextbfPrimal Problem   textbfDual Problem  \nmboxminimize  c^T x  quad quad  mboxminimize  b^T y  \nmboxsubject to  A x + s = b  quad quad  mboxsubject to  A^T y + c = 0 \n s in mathcalK    y in mathcalK^*\nendarray\nendsplit","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"where","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"x in R^n is the primal variable, y in R^m is the dual variable, and s in R^m is the primal slack","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"variable","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"mathcalK subseteq R^m is a closed convex cone and mathcalK^* subseteq R^m is the corresponding dual cone","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"variable","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"A in R^m times n, b in R^m, c in R^n are problem data","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"In the light of above, DiffOpt differentiates program variables x, s, y  w.r.t pertubations/sensivities in problem data i.e. dA, db, dc. This is achieved via implicit differentiation and matrix differential calculus.","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"Note that the primal (P) and dual (D) are self-duals of each other. Similarly for the constraints we support, mathcalK is same in format as mathcalK^*.","category":"page"},{"location":"manual/#Reference-articles","page":"Manual","title":"Reference articles","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"Differentiating Through a Cone Program - Akshay Agrawal, Shane Barratt, Stephen Boyd, Enzo Busseti, Walaa M. Moursi, 2019\nA fast and differentiable QP solver for PyTorch. Crafted by Brandon Amos and J. Zico Kolter.\nOptNet: Differentiable Optimization as a Layer in Neural Networks","category":"page"},{"location":"manual/#Backward-Pass-vector","page":"Manual","title":"Backward Pass vector","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"One possible point of confusion in finding jacobians is the role of the backward pass vector - above eqn (7), OptNet: Differentiable Optimization as a Layer in Neural Networks. While differentiating convex programs, it is often the case that we dont't want to find the acutal derivatives, rather we might be interested in  computing the product of jacobians with a backward pass vector, often used in backprop in machine learing/automatic differentiation. This is what happens in scheme 1 of DiffOpt backend.","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"But, for the conic system (scheme 2), we provide perturbations in conic data (dA, db, dc) to compute pertubations (dx, dy, dz) in input variables. Unlike the quadratic case, these perturbations are actual derivatives, not the product with a backward pass vector. This is an important distinction between the two schemes of differential optimization.","category":"page"},{"location":"examples/sensitivity-analysis-svm/","page":"Sensitivity Analysis of SVM","title":"Sensitivity Analysis of SVM","text":"EditURL = \"https://github.com/jump-dev/DiffOpt.jl/blob/master/docs/src/examples/sensitivity-analysis-svm.jl\"","category":"page"},{"location":"examples/sensitivity-analysis-svm/#Sensitivity-Analysis-of-SVM","page":"Sensitivity Analysis of SVM","title":"Sensitivity Analysis of SVM","text":"","category":"section"},{"location":"examples/sensitivity-analysis-svm/","page":"Sensitivity Analysis of SVM","title":"Sensitivity Analysis of SVM","text":"(Image: ) (Image: )","category":"page"},{"location":"examples/sensitivity-analysis-svm/","page":"Sensitivity Analysis of SVM","title":"Sensitivity Analysis of SVM","text":"This notebook illustrates sensitivity analysis of data points in a Support Vector Machine (inspired from @matbesancon's SimpleSVMs.)","category":"page"},{"location":"examples/sensitivity-analysis-svm/","page":"Sensitivity Analysis of SVM","title":"Sensitivity Analysis of SVM","text":"For reference, Section 10.1 of https://online.stat.psu.edu/stat508/book/export/html/792 gives an intuitive explanation of what it means to have a sensitive hyperplane or data point. The general form of the SVM training problem is given below (without regularization):","category":"page"},{"location":"examples/sensitivity-analysis-svm/","page":"Sensitivity Analysis of SVM","title":"Sensitivity Analysis of SVM","text":"beginsplit\nbeginarray ll\nmboxminimize  sum_i=1^N xi_i \nmboxst  xi_i ge 0 quad i=1N  \n             y_i (w^T X_i + b) ge 1 - xii\nendarray\nendsplit","category":"page"},{"location":"examples/sensitivity-analysis-svm/","page":"Sensitivity Analysis of SVM","title":"Sensitivity Analysis of SVM","text":"where","category":"page"},{"location":"examples/sensitivity-analysis-svm/","page":"Sensitivity Analysis of SVM","title":"Sensitivity Analysis of SVM","text":"X, y are the N data points\nξ is the soft-margin loss.","category":"page"},{"location":"examples/sensitivity-analysis-svm/#Define-and-solve-the-SVM","page":"Sensitivity Analysis of SVM","title":"Define and solve the SVM","text":"","category":"section"},{"location":"examples/sensitivity-analysis-svm/","page":"Sensitivity Analysis of SVM","title":"Sensitivity Analysis of SVM","text":"Import the libraries.","category":"page"},{"location":"examples/sensitivity-analysis-svm/","page":"Sensitivity Analysis of SVM","title":"Sensitivity Analysis of SVM","text":"using SCS, DiffOpt, LinearAlgebra, JuMP\nimport Random, Plots","category":"page"},{"location":"examples/sensitivity-analysis-svm/","page":"Sensitivity Analysis of SVM","title":"Sensitivity Analysis of SVM","text":"Construct separable, non-trivial data points.","category":"page"},{"location":"examples/sensitivity-analysis-svm/","page":"Sensitivity Analysis of SVM","title":"Sensitivity Analysis of SVM","text":"N = 100\nD = 2\nRandom.seed!(62)\nX = vcat(randn(N ÷ 2, D), randn(N ÷ 2, D) .+ [4.0, 1.5]')\ny = append!(ones(N ÷ 2), -ones(N ÷ 2));\nnothing #hide","category":"page"},{"location":"examples/sensitivity-analysis-svm/","page":"Sensitivity Analysis of SVM","title":"Sensitivity Analysis of SVM","text":"Let's initialize a special model that can understand sensitivities","category":"page"},{"location":"examples/sensitivity-analysis-svm/","page":"Sensitivity Analysis of SVM","title":"Sensitivity Analysis of SVM","text":"model = Model(() -> diff_optimizer(SCS.Optimizer))","category":"page"},{"location":"examples/sensitivity-analysis-svm/","page":"Sensitivity Analysis of SVM","title":"Sensitivity Analysis of SVM","text":"Add the variables","category":"page"},{"location":"examples/sensitivity-analysis-svm/","page":"Sensitivity Analysis of SVM","title":"Sensitivity Analysis of SVM","text":"@variable(model, l[1:N])\n@variable(model, w[1:D])\n@variable(model, b);\nnothing #hide","category":"page"},{"location":"examples/sensitivity-analysis-svm/","page":"Sensitivity Analysis of SVM","title":"Sensitivity Analysis of SVM","text":"Add the constraints.","category":"page"},{"location":"examples/sensitivity-analysis-svm/","page":"Sensitivity Analysis of SVM","title":"Sensitivity Analysis of SVM","text":"@constraint(\n    model,\n    1.0 * l ∈ MOI.Nonnegatives(N),\n)\n@constraint(\n    model,\n    cons,\n    y .* (X * w .+ b) + l .- 1 ∈ MOI.Nonnegatives(N),\n);\nnothing #hide","category":"page"},{"location":"examples/sensitivity-analysis-svm/","page":"Sensitivity Analysis of SVM","title":"Sensitivity Analysis of SVM","text":"Define the objective and solve","category":"page"},{"location":"examples/sensitivity-analysis-svm/","page":"Sensitivity Analysis of SVM","title":"Sensitivity Analysis of SVM","text":"@objective(\n    model,\n    Min,\n    sum(l),\n)\n\noptimize!(model) # solve","category":"page"},{"location":"examples/sensitivity-analysis-svm/","page":"Sensitivity Analysis of SVM","title":"Sensitivity Analysis of SVM","text":"We can visualize the separating hyperplane.","category":"page"},{"location":"examples/sensitivity-analysis-svm/","page":"Sensitivity Analysis of SVM","title":"Sensitivity Analysis of SVM","text":"loss = objective_value(model)\nwv = value.(w)\nbv = value(b)\n\nsvm_x = [0.0, 3.0]\nsvm_y = (-bv .- wv[1] * svm_x )/wv[2]\n\np = Plots.scatter(X[:,1], X[:,2], color = [yi > 0 ? :red : :blue for yi in y], label = \"\")\nPlots.yaxis!(p, (-2, 4.5))\nPlots.plot!(p, svm_x, svm_y, label = \"loss = $(round(loss, digits=2))\", width=3)","category":"page"},{"location":"examples/sensitivity-analysis-svm/#Experiments","page":"Sensitivity Analysis of SVM","title":"Experiments","text":"","category":"section"},{"location":"examples/sensitivity-analysis-svm/","page":"Sensitivity Analysis of SVM","title":"Sensitivity Analysis of SVM","text":"Now that we've solved the SVM, we can compute the sensitivity of optimal values – the separating hyperplane in our case – with respect to perturbations of the problem data – the data points – using DiffOpt. For illustration, we've explored two questions:","category":"page"},{"location":"examples/sensitivity-analysis-svm/","page":"Sensitivity Analysis of SVM","title":"Sensitivity Analysis of SVM","text":"How does a change in labels of the data points (y=1 to y=-1, and vice versa) affect the position of the hyperplane? This is achieved by finding the gradient of w, b with respect to y[i], the classification label of the ith data point.\nHow does a change in coordinates of the data points, X, affects the position of the hyperplane? This is achieved by finding gradient of w, b with respect to X[i], 2D coordinates of the data points.","category":"page"},{"location":"examples/sensitivity-analysis-svm/","page":"Sensitivity Analysis of SVM","title":"Sensitivity Analysis of SVM","text":"Note that finding the optimal SVM can be modelled as a conic optimization problem:","category":"page"},{"location":"examples/sensitivity-analysis-svm/","page":"Sensitivity Analysis of SVM","title":"Sensitivity Analysis of SVM","text":"beginalign*\n min_x in mathbbR^n  c^T x \n textst                A x + s = b  \n                            b in mathbbR^m  \n                            s in mathcalK\nendalign*","category":"page"},{"location":"examples/sensitivity-analysis-svm/","page":"Sensitivity Analysis of SVM","title":"Sensitivity Analysis of SVM","text":"where","category":"page"},{"location":"examples/sensitivity-analysis-svm/","page":"Sensitivity Analysis of SVM","title":"Sensitivity Analysis of SVM","text":"beginalign*\nc = l_1 - 1 l_2 -1  l_N -1 0 0  0 text(D+1 times) \n\nA =\nbeginbmatrix\n -l_1     0       0             0    0  0  \n    0  -l_2       0             0    0  0  \n                             0    0  0  \n    0     0    -l_N             0    0  0  \n    0     0       0  -y_1 X_11    -y_1 X_1N  -y_1  \n    0     0       0  -y_2 X_21    -y_1 X_2N  -y_2  \n                                                 \n    0     0       0  -y_N X_N1    -y_N X_NN  -y_N  \nendbmatrix \n\nb = 0 0  0 text(N times) l_1 - 1 l_2 -1  l_N -1 \n\nmathcalK = textSet of Nonnegative cones\nendalign*","category":"page"},{"location":"examples/sensitivity-analysis-svm/#Experiment-1:-Gradient-of-hyperplane-wrt-the-data-point-labels","page":"Sensitivity Analysis of SVM","title":"Experiment 1: Gradient of hyperplane wrt the data point labels","text":"","category":"section"},{"location":"examples/sensitivity-analysis-svm/","page":"Sensitivity Analysis of SVM","title":"Sensitivity Analysis of SVM","text":"Construct perturbations in data point labels y without changing the data point coordinates X.","category":"page"},{"location":"examples/sensitivity-analysis-svm/","page":"Sensitivity Analysis of SVM","title":"Sensitivity Analysis of SVM","text":"∇ = Float64[]\ndy = zeros(N);\nnothing #hide","category":"page"},{"location":"examples/sensitivity-analysis-svm/","page":"Sensitivity Analysis of SVM","title":"Sensitivity Analysis of SVM","text":"begin differentiating","category":"page"},{"location":"examples/sensitivity-analysis-svm/","page":"Sensitivity Analysis of SVM","title":"Sensitivity Analysis of SVM","text":"for Xi in 1:N\n    dy[Xi] = 1.0  # set\n\n    MOI.set(\n        model,\n        DiffOpt.ForwardInConstraint(),\n        cons,\n        MOI.Utilities.vectorize(dy .* MOI.SingleVariable(b)),\n    )\n\n    DiffOpt.forward(model)\n\n    dw = MOI.get.(\n        model,\n        DiffOpt.ForwardOutVariablePrimal(),\n        w\n    )\n    db = MOI.get(\n        model,\n        DiffOpt.ForwardOutVariablePrimal(),\n        b\n    )\n    push!(∇, norm(dw) + norm(db))\n\n    dy[Xi] = 0.0  # reset the change made above\nend\nnormalize!(∇);\nnothing #hide","category":"page"},{"location":"examples/sensitivity-analysis-svm/","page":"Sensitivity Analysis of SVM","title":"Sensitivity Analysis of SVM","text":"Visualize point sensitivities with respect to separating hyperplane. Note that the gradients are normalized.","category":"page"},{"location":"examples/sensitivity-analysis-svm/","page":"Sensitivity Analysis of SVM","title":"Sensitivity Analysis of SVM","text":"p2 = Plots.scatter(\n    X[:,1], X[:,2],\n    color = [yi > 0 ? :red : :blue for yi in y], label = \"\",\n    markersize = ∇ * 20,\n)\nPlots.yaxis!(p2, (-2, 4.5))\nPlots.plot!(p2, svm_x, svm_y, label = \"loss = $(round(loss, digits=2))\", width=3)","category":"page"},{"location":"examples/sensitivity-analysis-svm/#Experiment-2:-Gradient-of-hyperplane-wrt-the-data-point-coordinates","page":"Sensitivity Analysis of SVM","title":"Experiment 2: Gradient of hyperplane wrt the data point coordinates","text":"","category":"section"},{"location":"examples/sensitivity-analysis-svm/","page":"Sensitivity Analysis of SVM","title":"Sensitivity Analysis of SVM","text":"Similar to previous example, construct perturbations in data points coordinates X.","category":"page"},{"location":"examples/sensitivity-analysis-svm/","page":"Sensitivity Analysis of SVM","title":"Sensitivity Analysis of SVM","text":"∇ = Float64[]\ndX = zeros(N, D);\nnothing #hide","category":"page"},{"location":"examples/sensitivity-analysis-svm/","page":"Sensitivity Analysis of SVM","title":"Sensitivity Analysis of SVM","text":"begin differentiating","category":"page"},{"location":"examples/sensitivity-analysis-svm/","page":"Sensitivity Analysis of SVM","title":"Sensitivity Analysis of SVM","text":"for Xi in 1:N\n    dX[Xi, :] = ones(D)  # set\n\n    for i in 1:D\n        MOI.set(\n            model,\n            DiffOpt.ForwardInConstraint(),\n            cons,\n            MOI.Utilities.vectorize(dX[:,i] .* MOI.SingleVariable(w[i])),\n        )\n    end\n\n    DiffOpt.forward(model)\n\n    dw = MOI.get.(\n        model,\n        DiffOpt.ForwardOutVariablePrimal(),\n        w\n    )\n    db = MOI.get(\n        model,\n        DiffOpt.ForwardOutVariablePrimal(),\n        b\n    )\n    push!(∇, norm(dw) + norm(db))\n\n    dX[Xi, :] = zeros(D)  # reset the change made ago\nend\nnormalize!(∇);\nnothing #hide","category":"page"},{"location":"examples/sensitivity-analysis-svm/","page":"Sensitivity Analysis of SVM","title":"Sensitivity Analysis of SVM","text":"We can visualize point sensitivity with respect to the separating hyperplane. Note that the gradients are normalized.","category":"page"},{"location":"examples/sensitivity-analysis-svm/","page":"Sensitivity Analysis of SVM","title":"Sensitivity Analysis of SVM","text":"p3 = Plots.scatter(\n    X[:,1], X[:,2],\n    color = [yi > 0 ? :red : :blue for yi in y], label = \"\",\n    markersize = ∇ * 20,\n)\nPlots.yaxis!(p3, (-2, 4.5))\nPlots.plot!(p3, svm_x, svm_y, label = \"loss = $(round(loss, digits=2))\", width=3)","category":"page"},{"location":"examples/sensitivity-analysis-svm/","page":"Sensitivity Analysis of SVM","title":"Sensitivity Analysis of SVM","text":"","category":"page"},{"location":"examples/sensitivity-analysis-svm/","page":"Sensitivity Analysis of SVM","title":"Sensitivity Analysis of SVM","text":"This page was generated using Literate.jl.","category":"page"},{"location":"solve-QP/#Solving-QP-primal","page":"Solving a QP","title":"Solving QP primal","text":"","category":"section"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"using Random\nusing MathOptInterface\nusing Dualization\nusing OSQP\n\nconst MOI = MathOptInterface\nconst MOIU = MathOptInterface.Utilities;","category":"page"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"n = 20 # variable dimension\nm = 15 # no of inequality constraints\np = 15; # no of equality constraints","category":"page"},{"location":"solve-QP/#create-a-non-trivial-QP-problem","page":"Solving a QP","title":"create a non-trivial QP problem","text":"","category":"section"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"textmin  frac12x^TQx + q^Tx","category":"page"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"textst  Gx = h","category":"page"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"Ax = b ","category":"page"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"x̂ = rand(n)\nQ = rand(n, n)\nQ = Q'*Q # ensure PSD\nq = rand(n)\nG = rand(m, n)\nh = G*x̂ + rand(m)\nA = rand(p, n)\nb = A*x̂;","category":"page"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"model = MOI.instantiate(OSQP.Optimizer, with_bridge_type=Float64)\nx = MOI.add_variables(model, n);","category":"page"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"# define objective\n\nquad_terms = MOI.ScalarQuadraticTerm{Float64}[]\nfor i in 1:n\n    for j in i:n # indexes (i,j), (j,i) will be mirrored. specify only one kind\n        push!(quad_terms, MOI.ScalarQuadraticTerm(Q[i,j],x[i],x[j]))\n    end\nend\n\nobjective_function = MOI.ScalarQuadraticFunction(MOI.ScalarAffineTerm.(q, x),quad_terms,0.0)\nMOI.set(model, MOI.ObjectiveFunction{MOI.ScalarQuadraticFunction{Float64}}(), objective_function)\nMOI.set(model, MOI.ObjectiveSense(), MOI.MIN_SENSE)","category":"page"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"# maintain constrain to index map - will be useful later\nconstraint_map = Dict()\n\n# add constraints\nfor i in 1:m\n    ci = MOI.add_constraint(model,MOI.ScalarAffineFunction(MOI.ScalarAffineTerm.(G[i,:], x), 0.),MOI.LessThan(h[i]))\n    constraint_map[ci] = i\nend\n\nfor i in 1:p\n    ci = MOI.add_constraint(model,MOI.ScalarAffineFunction(MOI.ScalarAffineTerm.(A[i,:], x), 0.),MOI.EqualTo(b[i]))\n    constraint_map[ci] = i\nend","category":"page"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"MOI.optimize!(model)","category":"page"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"@assert MOI.get(model, MOI.TerminationStatus()) in [MOI.LOCALLY_SOLVED, MOI.OPTIMAL]","category":"page"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"x̄ = MOI.get(model, MOI.VariablePrimal(), x);","category":"page"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"# objective value (predicted vs actual) sanity check\n@assert 0.5*x̄'*Q*x̄ + q'*x̄  <= 0.5*x̂'*Q*x̂ + q'*x̂   ","category":"page"},{"location":"solve-QP/#find-and-solve-dual-problem","page":"Solving a QP","title":"find and solve dual problem","text":"","category":"section"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"primal dual\ntextmin  frac12x^TQx + q^Tx textmax  -frac12y^TQ^-1y - u^Th - v^Tb\ntextst  Gx = h textst   u geq 0 u in R^m v in R^n\nAx = b y = q + G^Tu + A^Tv","category":"page"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"Each primal variable becomes a dual constraint\nEach primal constraint becomes a dual variable","category":"page"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"# NOTE: can't use Ipopt\n# Ipopt.Optimizer doesn't supports accessing MOI.ObjectiveFunctionType\n\njoint_object    = dualize(model)\ndual_model_like = joint_object.dual_model # this is MOI.ModelLike, not an MOI.AbstractOptimizer; can't call optimizer on it\nprimal_dual_map = joint_object.primal_dual_map;","category":"page"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"# copy the dual model objective, constraints, and variables to an optimizer\ndual_model = MOI.instantiate(OSQP.Optimizer, with_bridge_type=Float64)\nMOI.copy_to(dual_model, dual_model_like)\n\n# solve dual\nMOI.optimize!(dual_model);","category":"page"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"# check if strong duality holds\n@assert abs(MOI.get(model, MOI.ObjectiveValue()) - MOI.get(dual_model, MOI.ObjectiveValue())) <= 1e-1","category":"page"},{"location":"solve-QP/#derive-and-verify-KKT-conditions","page":"Solving a QP","title":"derive and verify KKT conditions","text":"","category":"section"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"is_equality(set::S) where {S<:MOI.AbstractSet} = false\nis_equality(set::MOI.EqualTo{T}) where T = true\n\nmap = primal_dual_map.primal_con_dual_var;","category":"page"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"complimentary slackness: mu_i(Gbar x -h)_i=0 qquad text where  i=1m","category":"page"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"for con_index in keys(map)\n    # NOTE: OSQP.Optimizer doesn't allows access to MOI.ConstraintPrimal\n    #       That's why I defined a custom map \n    \n    set = MOI.get(model, MOI.ConstraintSet(), con_index)\n    μ   = MOI.get(dual_model, MOI.VariablePrimal(), map[con_index][1])\n    \n    if !is_equality(set)\n        # μ[i]*(Gx - h)[i] = 0\n        i = constraint_map[con_index]\n        \n        # println(μ,\" - \",G[i,:]'*x̄, \" - \",h[i])\n        # TODO: assertion fails \n        @assert μ*(G[i,:]'*x̄ - h[i]) < 1e-1  \n    end\nend","category":"page"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"primal feasibility:  (Gbar x -h)_i=0 qquad text where  i=1m (Abar x -b)_j=0 qquad text where  j=1p","category":"page"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"dual feasibility:  mu_i geq 0 qquad text where  i=1m","category":"page"},{"location":"solve-QP/","page":"Solving a QP","title":"Solving a QP","text":"for con_index in keys(map)\n    # NOTE: OSQP.Optimizer doesn't allows access to MOI.ConstraintPrimal\n    #       That's why I defined a custom map \n    \n    set = MOI.get(model, MOI.ConstraintSet(), con_index)\n    μ   = MOI.get(dual_model, MOI.VariablePrimal(), map[con_index][1])\n    i = constraint_map[con_index]\n    \n    if is_equality(set)\n        # (Ax - h)[i] = 0\n        @assert abs(A[i,:]'*x̄ - b[i]) < 1e-2\n    else\n        # (Gx - h)[i] = 0\n        @assert G[i,:]'*x̄ - h[i] < 1e-2\n        \n        # μ[i] >= 0\n        # TODO: assertion fails \n        @assert μ > -1e-2\n    end\nend","category":"page"},{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [DiffOpt]","category":"page"},{"location":"reference/#DiffOpt.AbstractLazyScalarFunction","page":"Reference","title":"DiffOpt.AbstractLazyScalarFunction","text":"abstract type AbstractLazyScalarFunction <: MOI.AbstractScalarFunction end\n\nSubtype of MOI.AbstractScalarFunction that is not a standard MOI scalar function but can be converted to one using standard_form.\n\nThe function can also be inspected lazily using JuMP.coefficient or quad_sym_half.\n\n\n\n\n\n","category":"type"},{"location":"reference/#DiffOpt.BackwardInVariablePrimal","page":"Reference","title":"DiffOpt.BackwardInVariablePrimal","text":"BackwardInVariablePrimal <: MOI.AbstractVariableAttribute\n\nA MOI.AbstractVariableAttribute to set input data to backward differentiation, that is, problem solution.\n\nFor instance, to set the tangent of the variable of index vi, do the following:\n\nMOI.set(model, DiffOpt.BackwardInVariablePrimal(), x)\n\n\n\n\n\n","category":"type"},{"location":"reference/#DiffOpt.BackwardOutConstraint","page":"Reference","title":"DiffOpt.BackwardOutConstraint","text":"BackwardOutConstraint\n\nAn MOI.AbstractConstraintAttribute to get output data to backward differentiation, that is, problem input data.\n\nFor instance, if the following returns x + 2y + 5, it means that the tangent has coordinate 1 for the coefficient of x, coordinate 2 for the coefficient of y and 5 for the function constant. If the constraint is of the form func == constant or func <= constant, the tangent for the constant on the right-hand side is -5.\n\nMOI.get(model, DiffOpt.BackwardOutConstraint(), ci)\n\n\n\n\n\n","category":"type"},{"location":"reference/#DiffOpt.BackwardOutObjective","page":"Reference","title":"DiffOpt.BackwardOutObjective","text":"BackwardOutObjective <: MOI.AbstractModelAttribute\n\nA MOI.AbstractModelAttribute to get output data to backward differentiation, that is, problem input data.\n\nFor instance, to get the tangent of the objective function corresponding to the tangent given to BackwardInVariablePrimal, do the following:\n\nfunc = MOI.get(model, DiffOpt.BackwardOutObjective)\n\nThen, to get the sensitivity of the linear term with variable x, do\n\nJuMP.coefficient(func, x)\n\nTo get the sensitivity with respect to the quadratic term with variables x and y, do either\n\nJuMP.coefficient(func, x, y)\n\nor\n\nDiffOpt.quad_sym_half(func, x, y)\n\nwarning: Warning\nThese two lines are not equivalent in case x == y, see quad_sym_half for the details on the difference between these two functions.\n\n\n\n\n\n","category":"type"},{"location":"reference/#DiffOpt.ForwardInConstraint","page":"Reference","title":"DiffOpt.ForwardInConstraint","text":"ForwardInConstraint <: MOI.AbstractConstraintAttribute\n\nA MOI.AbstractConstraintAttribute to set input data to forward differentiation, that is, problem input data.\n\nFor instance, if the scalar constraint of index ci contains θ * (x + 2y) <= 5θ, for the purpose of computing the derivative with respect to θ, the following should be set:\n\nfx = MOI.SingleVariable(x)\nfy = MOI.SingleVariable(y)\nMOI.set(model, DiffOpt.ForwardInConstraint(), ci, 1.0 * fx + 2.0 * fy - 5.0)\n\nNote that we use -5 as the ForwardInConstraint sets the tangent of the ConstraintFunction so we consider the expression θ * (x + 2y - 5).\n\n\n\n\n\n","category":"type"},{"location":"reference/#DiffOpt.ForwardInObjective","page":"Reference","title":"DiffOpt.ForwardInObjective","text":"ForwardInObjective <: MOI.AbstractModelAttribute\n\nA MOI.AbstractModelAttribute to set input data to forward differentiation, that is, problem input data. The possible values are any MOI.AbstractScalarFunction. A MOI.ScalarQuadraticFunction can only be used in linearly constrained quadratic models.\n\nFor instance, if the objective contains θ * (x + 2y), for the purpose of computinig the derivative with respect to θ, the following should be set:\n\nfx = MOI.SingleVariable(x)\nfy = MOI.SingleVariable(y)\nMOI.set(model, DiffOpt.ForwardInObjective(), 1.0 * fx + 2.0 * fy)\n\nwhere x and y are the relevant MOI.VariableIndex.\n\n\n\n\n\n","category":"type"},{"location":"reference/#DiffOpt.ForwardOutVariablePrimal","page":"Reference","title":"DiffOpt.ForwardOutVariablePrimal","text":"ForwardOutVariablePrimal <: MOI.AbstractVariableAttribute\n\nA MOI.AbstractVariableAttribute to get output data from forward differentiation, that is, problem solution.\n\nFor instance, to get the tangent of the variable of index vi corresponding to the tangents given to ForwardInObjective and ForwardInConstraint, do the following:\n\nMOI.get(model, DiffOpt.ForwardOutVariablePrimal(), vi)\n\n\n\n\n\n","category":"type"},{"location":"reference/#DiffOpt.IndexMappedFunction","page":"Reference","title":"DiffOpt.IndexMappedFunction","text":"IndexMappedFunction{F<:MOI.AbstractFunction} <: AbstractLazyScalarFunction\n\nLazily represents the function MOI.Utilities.map_indices(index_map, DiffOpt.standard_form(func)).\n\n\n\n\n\n","category":"type"},{"location":"reference/#DiffOpt.MOItoJuMP","page":"Reference","title":"DiffOpt.MOItoJuMP","text":"MOItoJuMP{F<:MOI.AbstractScalarFunction} <: JuMP.AbstractJuMPScalar\n\nLazily represents the function JuMP.jump_function(model, DiffOpt.standard_form(func)).\n\n\n\n\n\n","category":"type"},{"location":"reference/#DiffOpt.MatrixScalarQuadraticFunction","page":"Reference","title":"DiffOpt.MatrixScalarQuadraticFunction","text":"struct MatrixScalarQuadraticFunction{T, VT, MT} <: MOI.AbstractScalarFunction\n    affine::VectorScalarAffineFunction{T,VT}\n    terms::MT\nend\n\nRepresents the function x' * terms * x / 2 + affine as an MOI.AbstractScalarFunction where x[i] = MOI.VariableIndex(i). Use standard_form to convert it to a MOI.ScalarQuadraticFunction{T}.\n\n\n\n\n\n","category":"type"},{"location":"reference/#DiffOpt.MatrixVectorAffineFunction","page":"Reference","title":"DiffOpt.MatrixVectorAffineFunction","text":"MatrixVectorAffineFunction{T, VT} <: MOI.AbstractVectorFunction\n\nRepresents the function terms * x + constant as an MOI.AbstractVectorFunction where x[i] = MOI.VariableIndex(i). Use standard_form to convert it to a MOI.VectorAffineFunction{T}.\n\n\n\n\n\n","category":"type"},{"location":"reference/#DiffOpt.ProgramClass","page":"Reference","title":"DiffOpt.ProgramClass","text":"ProgramClass <: MOI.AbstractOptimizerAttribute\n\nDetermines which program class to used from ProgramClassCode. The default is AUTOMATIC.\n\nOne important advantage of setting the class explicitly is that it will allow necessary bridges to be used. If the class is AUTOMATIC then DiffOpt.Optimizer will report that it supports both objective and constraints of the QP and CP classes. For instance, it will reports that is supports both quadratic objective and conic constraints. However, at the differentiation stage, we won't be able to differentiate since QP does not support conic constraints and CP does not support quadratic objective. On the other hand, if the ProgramClass is set to CONIC then DiffOpt.Optimizer will report that it does not support quadratic objective hence it will be bridged to second-order cone constraints and we will be able to use CP to differentiate.\n\n\n\n\n\n","category":"type"},{"location":"reference/#DiffOpt.ProgramClassCode","page":"Reference","title":"DiffOpt.ProgramClassCode","text":"@enum ProgramClassCode QUADRATIC CONIC AUTOMATIC\n\nProgram class used by DiffOpt. DiffOpt implements differentiation of two different program class:\n\nQuadratic Program (QP): quadratic objective and linear constraints and\nConic Program (CP): linear objective and conic constraints.\n\nAUTOMATIC which means that the class will be automatically selected given the problem data: if any constraint is conic, CP is used and QP is used otherwise. See ProgramClass.\n\n\n\n\n\n","category":"type"},{"location":"reference/#DiffOpt.ProgramClassUsed","page":"Reference","title":"DiffOpt.ProgramClassUsed","text":"ProgramClassUsed <: MOI.AbstractOptimizerAttribute\n\nProgram class actually used, same as ProgramClass except that it does not return AUTOMATIC but the class automatically chosen instead. This attribute is read-only, it cannot be set, set ProgramClass instead.\n\n\n\n\n\n","category":"type"},{"location":"reference/#DiffOpt.VectorScalarAffineFunction","page":"Reference","title":"DiffOpt.VectorScalarAffineFunction","text":"VectorScalarAffineFunction{T, VT} <: MOI.AbstractScalarFunction\n\nRepresents the function x ⋅ terms + constant as an MOI.AbstractScalarFunction where x[i] = MOI.VariableIndex(i). Use standard_form to convert it to a MOI.ScalarAffineFunction{T}.\n\n\n\n\n\n","category":"type"},{"location":"reference/#DiffOpt.Dπ-Union{Tuple{T}, Tuple{Vector{T}, MathOptInterface.ModelLike, MatrixOptInterface.GeometricConicForm, MathOptInterface.Utilities.IndexMap}} where T","page":"Reference","title":"DiffOpt.Dπ","text":"Dπ(v::Vector{Float64}, model, conic_form::MatOI.GeometricConicForm, index_map::MOIU.IndexMap)\n\nGiven a model, its conic_form and the index_map from the indices of model to the indices of conic_form, find the gradient of the projection of the vectors v of length equal to the number of rows in the conic form onto the cartesian product of the cones corresponding to these rows. For more info, refer to https://github.com/matbesancon/MathOptSetDistances.jl\n\n\n\n\n\n","category":"method"},{"location":"reference/#DiffOpt._backward_conic-Tuple{DiffOpt.Optimizer}","page":"Reference","title":"DiffOpt._backward_conic","text":"_backward_conic(model::Optimizer, dx::Vector{Float64}, dy::Vector{Float64}, ds::Vector{Float64})\n\nMethod to compute the product of the transpose of the derivative (Jacobian) at the conic program parameters A, b, c  to the perturbations dx, dy, ds. This is similar to backward.\n\nFor theoretical background, refer Section 3 of Differentiating Through a Cone Program, https://arxiv.org/abs/1904.09043\n\n\n\n\n\n","category":"method"},{"location":"reference/#DiffOpt._backward_quad-Tuple{DiffOpt.Optimizer}","page":"Reference","title":"DiffOpt._backward_quad","text":"_backward_quad(model::Optimizer)\n\nMethod to differentiate optimal solution z and return product of jacobian matrices (dz / dQ, dz / dq, etc) with the backward pass vector dl / dz\n\nThe method computes the product of\n\njacobian of problem solution z* with respect to  problem parameters set with the BackwardInVariablePrimal\na backward pass vector dl / dz, where l can be a loss function\n\nNote that this method does not returns the actual jacobians.\n\nFor more info refer eqn(7) and eqn(8) of https://arxiv.org/pdf/1703.00443.pdf\n\n\n\n\n\n","category":"method"},{"location":"reference/#DiffOpt._forward_conic-Tuple{DiffOpt.Optimizer}","page":"Reference","title":"DiffOpt._forward_conic","text":"_forward_conic(model::Optimizer)\n\nMethod to compute the product of the derivative (Jacobian) at the conic program parameters A, b, c  to the perturbations dA, db, dc. This is similar to forward.\n\nFor theoretical background, refer Section 3 of Differentiating Through a Cone Program, https://arxiv.org/abs/1904.09043\n\n\n\n\n\n","category":"method"},{"location":"reference/#DiffOpt._forward_quad-Tuple{DiffOpt.Optimizer}","page":"Reference","title":"DiffOpt._forward_quad","text":"_forward_quad(model::Optimizer)\n\n\n\n\n\n","category":"method"},{"location":"reference/#DiffOpt.backward-Tuple{DiffOpt.Optimizer}","page":"Reference","title":"DiffOpt.backward","text":"backward(model::Optimizer)\n\nWrapper method for the backward pass. This method will consider as input a currently solved problem and differentials with respect to the solution set with the BackwardInVariablePrimal attribute. The output problem data differentials can be queried with the attributes BackwardOutObjective and BackwardOutConstraint.\n\n\n\n\n\n","category":"method"},{"location":"reference/#DiffOpt.create_LHS_matrix","page":"Reference","title":"DiffOpt.create_LHS_matrix","text":"create_LHS_matrix(z, λ, Q, G, h, A=nothing)\n\nInverse matrix specified on RHS of eqn(7) in https://arxiv.org/pdf/1703.00443.pdf\n\nHelper method while calling _backward_quad\n\n\n\n\n\n","category":"function"},{"location":"reference/#DiffOpt.diff_optimizer-Tuple{Any}","page":"Reference","title":"DiffOpt.diff_optimizer","text":"diff_optimizer(optimizer_constructor)::Optimizer\n\nCreates a DiffOpt.Optimizer, which is an MOI layer with an internal optimizer and other utility methods. Results (primal, dual and slack values) are obtained by querying the internal optimizer instantiated using the optimizer_constructor. These values are required for find jacobians with respect to problem data.\n\nOne define a differentiable model by using any solver of choice. Example:\n\njulia> using DiffOpt, GLPK\n\njulia> model = diff_optimizer(GLPK.Optimizer)\njulia> model.add_variable(x)\njulia> model.add_constraint(...)\n\njulia> _backward_quad(model)  # for convex quadratic models\n\njulia> _backward_quad(model)  # for convex conic models\n\n\n\n\n\n","category":"method"},{"location":"reference/#DiffOpt.forward-Tuple{DiffOpt.Optimizer}","page":"Reference","title":"DiffOpt.forward","text":"forward(model::Optimizer)\n\nWrapper method for the forward pass. This method will consider as input a currently solved problem and differentials with respect to problem data set with the ForwardInObjective and  ForwardInConstraint attributes. The output solution differentials can be queried with the attribute ForwardOutVariablePrimal.\n\n\n\n\n\n","category":"method"},{"location":"reference/#DiffOpt.get_problem_data-Tuple{MathOptInterface.AbstractOptimizer}","page":"Reference","title":"DiffOpt.get_problem_data","text":"get_problem_data(model::MOI.AbstractOptimizer)\n\nReturn problem parameters as matrices along with other program info such as number of constraints, variables, etc\n\n\n\n\n\n","category":"method"},{"location":"reference/#DiffOpt.map_rows-Tuple{Function, Any, MatrixOptInterface.GeometricConicForm, MathOptInterface.Utilities.IndexMap, Union{DiffOpt.Flattened, DiffOpt.Nested}}","page":"Reference","title":"DiffOpt.map_rows","text":"map_rows(f::Function, model, conic_form::MatOI.GeometricConicForm, index_map::MOIU.IndexMap, map_mode::Union{Nested{T}, Flattened{T}})\n\nGiven a model, its conic_form, the index_map from the indices of model to the indices of conic_form and map_mode of type Nested (resp. Flattened), return a Vector{T} of length equal to the number of cones (resp. rows) in the conic form where the value for the index (resp. rows) corresponding to each cone is equal to f(ci, r) where ci is the corresponding constraint index in model and r is a UnitRange of the corresponding rows in the conic form.\n\n\n\n\n\n","category":"method"},{"location":"reference/#DiffOpt.quad_sym_half","page":"Reference","title":"DiffOpt.quad_sym_half","text":"quad_sym_half(func, vi1::MOI.VariableIndex, vi2::MOI.VariableIndex)\n\nReturn Q[i,j] = Q[j,i] where the quadratic terms of func is represented by x' Q x / 2 for a symmetric matrix Q where x[i] = vi1 and x[j] = vi2. Note that while this is equal to JuMP.coefficient(func, vi1, vi2) if vi1 != vi2, in the case vi1 == vi2, it is rather equal to 2JuMP.coefficient(func, vi1, vi2).\n\n\n\n\n\n","category":"function"},{"location":"reference/#DiffOpt.standard_form","page":"Reference","title":"DiffOpt.standard_form","text":"standard_form(func::AbstractLazyScalarFunction)\n\nConverts func to a standard MOI scalar function.\n\nstandard_form(func::MOItoJuMP)\n\nConverts func to a standard JuMP scalar function.\n\n\n\n\n\n","category":"function"},{"location":"reference/#DiffOpt.π-Union{Tuple{T}, Tuple{Vector{T}, MathOptInterface.ModelLike, MatrixOptInterface.GeometricConicForm, MathOptInterface.Utilities.IndexMap}} where T","page":"Reference","title":"DiffOpt.π","text":"π(v::Vector{Float64}, model::MOI.ModelLike, conic_form::MatOI.GeometricConicForm, index_map::MOIU.IndexMap)\n\nGiven a model, its conic_form and the index_map from the indices of model to the indices of conic_form, find the projection of the vectors v of length equal to the number of rows in the conic form onto the cartesian product of the cones corresponding to these rows. For more info, refer to https://github.com/matbesancon/MathOptSetDistances.jl\n\n\n\n\n\n","category":"method"},{"location":"solve-LP/#Solving-LP-primal","page":"Solving an LP","title":"Solving LP primal","text":"","category":"section"},{"location":"solve-LP/","page":"Solving an LP","title":"Solving an LP","text":"using Random\nusing GLPK\nusing MathOptInterface\nusing Dualization\n\nconst MOI  = MathOptInterface\nconst MOIU = MathOptInterface.Utilities;","category":"page"},{"location":"solve-LP/","page":"Solving an LP","title":"Solving an LP","text":"D = 10  # variable dimension\nN = 20; # no of inequality constraints","category":"page"},{"location":"solve-LP/#create-a-non-trivial-LP-problem","page":"Solving an LP","title":"create a non-trivial LP problem","text":"","category":"section"},{"location":"solve-LP/","page":"Solving an LP","title":"Solving an LP","text":"textmin  c^Tx","category":"page"},{"location":"solve-LP/","page":"Solving an LP","title":"Solving an LP","text":"textst  Ax leq b","category":"page"},{"location":"solve-LP/","page":"Solving an LP","title":"Solving an LP","text":"$$x \\geq 0, x \\in R^D$$","category":"page"},{"location":"solve-LP/","page":"Solving an LP","title":"Solving an LP","text":"s = rand(N)\ns = 2*s.-1\nλ = max.(-s, 0)\ns = max.(s, 0)\nx̂ = rand(D)\nA = rand(N, D)\nb = A*x̂ .+ s\nc = -A'*λ;","category":"page"},{"location":"solve-LP/","page":"Solving an LP","title":"Solving an LP","text":"# can feed dual problem to optimizer like this:\n# model = MOI.instantiate(dual_optimizer(GLPK.Optimizer), with_bridge_type=Float64)\n\nmodel = GLPK.Optimizer()\nx = MOI.add_variables(model, D)\n\n# define objective\nobjective_function = MOI.ScalarAffineFunction(MOI.ScalarAffineTerm.(c, x), 0.0)\nMOI.set(model, MOI.ObjectiveFunction{MOI.ScalarAffineFunction{Float64}}(), objective_function)\nMOI.set(model, MOI.ObjectiveSense(), MOI.MIN_SENSE)","category":"page"},{"location":"solve-LP/","page":"Solving an LP","title":"Solving an LP","text":"# will be useful later\nconstraint_indices = []\n\n# set constraints\nfor i in 1:N\n    push!(constraint_indices, MOI.add_constraint(model,MOI.ScalarAffineFunction(MOI.ScalarAffineTerm.(A[i,:], x), 0.),MOI.LessThan(b[i])))\nend\n\nfor i in 1:D\n    push!(constraint_indices, MOI.add_constraint(model,MOI.SingleVariable(x[i]),MOI.GreaterThan(0.)))\nend","category":"page"},{"location":"solve-LP/","page":"Solving an LP","title":"Solving an LP","text":"MOI.optimize!(model)","category":"page"},{"location":"solve-LP/","page":"Solving an LP","title":"Solving an LP","text":"@assert MOI.get(model, MOI.TerminationStatus()) in [MOI.LOCALLY_SOLVED, MOI.OPTIMAL]","category":"page"},{"location":"solve-LP/","page":"Solving an LP","title":"Solving an LP","text":"x̄ = MOI.get(model, MOI.VariablePrimal(), x);  # solution","category":"page"},{"location":"solve-LP/","page":"Solving an LP","title":"Solving an LP","text":"@assert abs(c'x̄ - c'x̂) <= 1e-8   # sanity check","category":"page"},{"location":"solve-LP/#find-and-solve-dual-problem","page":"Solving an LP","title":"find and solve dual problem","text":"","category":"section"},{"location":"solve-LP/","page":"Solving an LP","title":"Solving an LP","text":"primal dual\ntextmin  c^Tx textmax  b^Ty\ntextst  Ax leq b textst  A^Ty geq c\nx geq 0 y leq 0","category":"page"},{"location":"solve-LP/","page":"Solving an LP","title":"Solving an LP","text":"Each primal variable becomes a dual constraint\nEach primal constraint becomes a dual variable","category":"page"},{"location":"solve-LP/","page":"Solving an LP","title":"Solving an LP","text":"joint_object    = dualize(model)\ndual_model_like = joint_object.dual_model # this is MOI.ModelLike, not an MOI.AbstractOptimizer; can't call optimizer on it\nprimal_dual_map = joint_object.primal_dual_map;","category":"page"},{"location":"solve-LP/","page":"Solving an LP","title":"Solving an LP","text":"# copy the dual model objective, constraints, and variables to an optimizer\ndual_model = GLPK.Optimizer()\nMOI.copy_to(dual_model, dual_model_like)\n\n# solve dual\nMOI.optimize!(dual_model);","category":"page"},{"location":"solve-LP/","page":"Solving an LP","title":"Solving an LP","text":"# NOTE: You can obtain components of the dual model individually by -\n# dual_objective = dual_model_like.objective  # b'y\n# dual_variable_indices = [primal_dual_map.primal_con_dual_var[x][1] for x in constraint_indices]\n# dual_constraint_indices = [primal_dual_map.primal_var_dual_con[i] for i in x];\n\n# ŷ = MOI.get(dm, MOI.VariablePrimal(), dual_variable_indices)","category":"page"},{"location":"solve-LP/","page":"Solving an LP","title":"Solving an LP","text":"# check if strong duality holds\n@assert abs(MOI.get(model, MOI.ObjectiveValue()) - MOI.get(dual_model, MOI.ObjectiveValue())) <= 1e-8","category":"page"},{"location":"solve-LP/#derive-and-verify-KKT-conditions","page":"Solving an LP","title":"derive and verify KKT conditions","text":"","category":"section"},{"location":"solve-LP/","page":"Solving an LP","title":"Solving an LP","text":"complimentary slackness: mu_i(Abar x -b)_i=0quad mu_j+N bar x_j =0 qquad text where  i=1N j = 1D","category":"page"},{"location":"solve-LP/","page":"Solving an LP","title":"Solving an LP","text":"is_less_than(set::S) where {S<:MOI.AbstractSet} = false\nis_less_than(set::MOI.LessThan{T}) where T = true\n\nmap = primal_dual_map.primal_con_dual_var\nfor con_index in keys(map)\n    con_value = MOI.get(model, MOI.ConstraintPrimal(), con_index)\n    set = MOI.get(model, MOI.ConstraintSet(), con_index)\n    μ         = MOI.get(dual_model, MOI.VariablePrimal(), map[con_index][1])\n    \n    if is_less_than(set)\n        # μ[i]*(Ax - b)[i] = 0\n        @assert μ*(con_value - set.upper) < 1e-10\n    else\n        # μ[j]*x[j] = 0\n        @assert μ*(con_value - set.lower) < 1e-10\n    end\nend","category":"page"},{"location":"solve-LP/","page":"Solving an LP","title":"Solving an LP","text":"","category":"page"},{"location":"chainrules_unit/#ChainRules-integration","page":"ChainRules integration","title":"ChainRules integration","text":"","category":"section"},{"location":"chainrules_unit/","page":"ChainRules integration","title":"ChainRules integration","text":"In this example, we will demonstrate the integration of DiffOpt with ChainRulesCore.jl, the library allowing the definition of derivatives for functions that can then be used by automatic differentiation systems.","category":"page"},{"location":"chainrules_unit/","page":"ChainRules integration","title":"ChainRules integration","text":"using DiffOpt, Plots, JuMP\nusing LinearAlgebra\nimport Clp","category":"page"},{"location":"chainrules_unit/","page":"ChainRules integration","title":"ChainRules integration","text":"using ChainRulesCore","category":"page"},{"location":"chainrules_unit/#Unit-commitment-problem","page":"ChainRules integration","title":"Unit commitment problem","text":"","category":"section"},{"location":"chainrules_unit/","page":"ChainRules integration","title":"ChainRules integration","text":"We will consider a unit commitment problem, finding the cost-minimizing activation of generation units in a power network over multiple time periods. The considered constraints include:","category":"page"},{"location":"chainrules_unit/","page":"ChainRules integration","title":"ChainRules integration","text":"Demand satisfaction of several loads\nRamping constraints\nGeneration limits.","category":"page"},{"location":"chainrules_unit/","page":"ChainRules integration","title":"ChainRules integration","text":"The decisions are:","category":"page"},{"location":"chainrules_unit/","page":"ChainRules integration","title":"ChainRules integration","text":"u_it in 01: activation of the i-th unit at time t\np_it: power output of the i-th unit at time t.","category":"page"},{"location":"chainrules_unit/","page":"ChainRules integration","title":"ChainRules integration","text":"DiffOpt handles convex optimization problems only, we therefore relax the domain of the u_it variables to left01right.","category":"page"},{"location":"chainrules_unit/#Primal-UC-problem","page":"ChainRules integration","title":"Primal UC problem","text":"","category":"section"},{"location":"chainrules_unit/","page":"ChainRules integration","title":"ChainRules integration","text":"ChainRules defines the differentiation of functions. The actual function that is differentiated in the context of DiffOpt is the solution map taking in input the problem parameters and returning the solution.","category":"page"},{"location":"chainrules_unit/","page":"ChainRules integration","title":"ChainRules integration","text":"function unit_commitment(\n        load1_demand, load2_demand, gen_costs, noload_costs;\n        model = Model(Clp.Optimizer), silent=false)\n    MOI.set(model, MOI.Silent(), silent)\n\n    ## Problem data\n    unit_codes = [1, 2] # Generator identifiers\n    load_names = [\"Load1\", \"Load2\"] # Load identifiers\n    n_periods = 4 # Number of time periods\n    Pmin = Dict(1 => fill(0.5, n_periods), 2 => fill(0.5, n_periods)) # Minimum power output (pu)\n    Pmax = Dict(1 => fill(3.0, n_periods), 2 => fill(3.0, n_periods)) # Maximum power output (pu)\n    RR = Dict(1 => 0.25, 2 => 0.25) # Ramp rates (pu/min)\n    P0 = Dict(1 => 0.0, 2 => 0.0) # Initial power output (pu)\n    D = Dict(\"Load1\" => load1_demand, \"Load2\" => load2_demand) # Demand (pu)\n    Cp = Dict(1 => gen_costs[1], 2 => gen_costs[2]) # Generation cost coefficient ($/pu)\n    Cnl = Dict(1 => noload_costs[1], 2 => noload_costs[2]) # No-load cost ($)\n\n    ## Variables\n    # Note: u represents the activation of generation units.\n    # Would be binary in the typical UC problem, relaxed here to u ∈ [0,1]\n    # for a linear relaxation.\n    @variable(model, 0 <= u[g in unit_codes, t in 1:n_periods] <= 1) # Commitment\n    @variable(model, p[g in unit_codes, t in 1:n_periods] >= 0) # Power output (pu)\n\n    ## Constraints\n\n    # Energy balance\n    @constraint(\n        model,\n        energy_balance_cons[t in 1:n_periods],\n        sum(p[g, t] for g in unit_codes) == sum(D[l][t] for l in load_names),\n    )\n\n    # Generation limits\n    @constraint(model, [g in unit_codes, t in 1:n_periods], Pmin[g][t] * u[g, t] <= p[g, t])\n    @constraint(model, [g in unit_codes, t in 1:n_periods], p[g, t] <= Pmax[g][t] * u[g, t])\n\n    # Ramp rates\n    @constraint(model, [g in unit_codes, t in 2:n_periods], p[g, t] - p[g, t - 1] <= 60 * RR[g])\n    @constraint(model, [g in unit_codes], p[g, 1] - P0[g] <= 60 * RR[g])\n    @constraint(model, [g in unit_codes, t in 2:n_periods], p[g, t - 1] - p[g, t] <= 60 * RR[g])\n    @constraint(model, [g in unit_codes], P0[g] - p[g, 1] <= 60 * RR[g])\n\n    # Objective\n    @objective(\n        model,\n        Min,\n        sum((Cp[g] * p[g, t]) + (Cnl[g] * u[g, t]) for g in unit_codes, t in 1:n_periods),\n    )\n\n    optimize!(model)\n    # asserting finite optimal value\n    @assert termination_status(model) == MOI.OPTIMAL\n    # converting to dense matrix\n    return JuMP.value.(p.data)\nend","category":"page"},{"location":"chainrules_unit/","page":"ChainRules integration","title":"ChainRules integration","text":"m = Model(Clp.Optimizer)\n@show unit_commitment(\n    [1.0, 1.2, 1.4, 1.6], [1.0, 1.2, 1.4, 1.6],\n    [1000.0, 1500.0], [500.0, 1000.0],\n    model=m, silent=true\n)\nprintln(m)","category":"page"},{"location":"chainrules_unit/#Perturbation-of-a-single-input-parameter","page":"ChainRules integration","title":"Perturbation of a single input parameter","text":"","category":"section"},{"location":"chainrules_unit/","page":"ChainRules integration","title":"ChainRules integration","text":"Let us vary the demand at the second time frame on both loads:","category":"page"},{"location":"chainrules_unit/","page":"ChainRules integration","title":"ChainRules integration","text":"demand_values = 0.05:0.05:3.0\npvalues = map(demand_values) do di\n    unit_commitment(\n        [1.0, di, 1.4, 1.6], [1.0, di, 1.4, 1.6],\n        [1000.0, 1500.0], [500.0, 1000.0],\n        silent=true,\n    )\nend\npflat = [getindex.(pvalues, i) for i in eachindex(pvalues[1])]","category":"page"},{"location":"chainrules_unit/","page":"ChainRules integration","title":"ChainRules integration","text":"The influence of this variation of the demand is piecewise linear on the generation at different time frames:","category":"page"},{"location":"chainrules_unit/","page":"ChainRules integration","title":"ChainRules integration","text":"scatter(demand_values, pflat)\ntitle!(\"Generation at different time frames and generators for a single variation\")\nxlims!(0.0, 3.5)","category":"page"},{"location":"chainrules_unit/#Forward-Differentiation","page":"ChainRules integration","title":"Forward Differentiation","text":"","category":"section"},{"location":"chainrules_unit/","page":"ChainRules integration","title":"ChainRules integration","text":"Forward differentiation rule for the solution map of the unit commitment problem. It takes as arguments:","category":"page"},{"location":"chainrules_unit/","page":"ChainRules integration","title":"ChainRules integration","text":"the perturbations on the input parameters\nthe differentiated function\nthe primal values of the input parameters,","category":"page"},{"location":"chainrules_unit/","page":"ChainRules integration","title":"ChainRules integration","text":"and returns a tuple (primal_output, perturbations), the main primal result and the perturbation propagated to this result:","category":"page"},{"location":"chainrules_unit/","page":"ChainRules integration","title":"ChainRules integration","text":"function ChainRulesCore.frule(\n        (_, Δload1_demand, Δload2_demand, Δgen_costs, Δnoload_costs),\n        ::typeof(unit_commitment),\n        load1_demand, load2_demand, gen_costs, noload_costs;\n        optimizer=Clp.Optimizer,\n        )\n    # creating the UC model with a DiffOpt optimizer wrapper around Clp\n    model = Model(() -> diff_optimizer(optimizer))\n    # building and solving the main model\n    pv = unit_commitment(load1_demand, load2_demand, gen_costs, noload_costs, model=model)\n    energy_balance_cons = model[:energy_balance_cons]\n\n    # Setting some perturbation of the energy balance constraints\n    # Perturbations are set as MOI functions\n    Δenergy_balance = [\n        convert(MOI.ScalarAffineFunction{Float64}, d1 + d2)\n        for (d1, d2) in zip(Δload1_demand, Δload2_demand)\n    ]\n    MOI.set.(\n        model,\n        DiffOpt.ForwardInConstraint(), energy_balance_cons,\n        Δenergy_balance,\n    )\n\n    p = model[:p]\n    u = model[:u]\n\n    # setting the perturbation of the linear objective\n    Δobj = sum(Δgen_costs ⋅ p[:,t] + Δnoload_costs ⋅ u[:,t] for t in size(p, 2))\n    MOI.set(model, DiffOpt.ForwardInObjective(), Δobj)\n\n    DiffOpt.forward(JuMP.backend(model))\n    # querying the corresponding perturbation of the decision\n    Δp = MOI.get.(model, DiffOpt.ForwardOutVariablePrimal(), p)\n    return (pv, Δp.data)\nend","category":"page"},{"location":"chainrules_unit/","page":"ChainRules integration","title":"ChainRules integration","text":"We can now compute the perturbation of the output powers Δpv for a perturbation of the first load demand at time 2:","category":"page"},{"location":"chainrules_unit/","page":"ChainRules integration","title":"ChainRules integration","text":"load1_demand = [1.0, 1.0, 1.4, 1.6]\nload2_demand = [1.0, 1.0, 1.4, 1.6]\ngen_costs = [1000.0, 1500.0]\nnoload_costs = [500.0, 1000.0]\n\n# all input perturbations are 0\n# except first load at time 2\nΔload1_demand = 0 * load1_demand\nΔload1_demand[2] = 1.0\nΔload2_demand = 0 * load2_demand\nΔgen_costs = 0 * gen_costs\nΔnoload_costs = 0 * noload_costs\n(pv, Δpv) = ChainRulesCore.frule(\n    (nothing, Δload1_demand, Δload2_demand, Δgen_costs, Δnoload_costs),\n    unit_commitment,\n    load1_demand, load2_demand, gen_costs, noload_costs,\n)","category":"page"},{"location":"chainrules_unit/","page":"ChainRules integration","title":"ChainRules integration","text":"Δpv","category":"page"},{"location":"chainrules_unit/","page":"ChainRules integration","title":"ChainRules integration","text":"The result matches what we observe in the previous figure: the generation of the first generator at the second time frame (third element on the plot).","category":"page"},{"location":"chainrules_unit/#Reverse-mode-differentiation-of-the-solution-map","page":"ChainRules integration","title":"Reverse-mode differentiation of the solution map","text":"","category":"section"},{"location":"chainrules_unit/","page":"ChainRules integration","title":"ChainRules integration","text":"The rrule returns the primal and a pullback. The pullback takes a seed for the optimal solution ̄p and returns derivatives with respect to each input parameter of the function.","category":"page"},{"location":"chainrules_unit/","page":"ChainRules integration","title":"ChainRules integration","text":"function ChainRulesCore.rrule(\n        ::typeof(unit_commitment),\n        load1_demand, load2_demand, gen_costs, noload_costs;\n        optimizer=Clp.Optimizer,\n        silent=false)\n    model = Model(() -> diff_optimizer(optimizer))\n    # solve the forward UC problem\n    pv = unit_commitment(load1_demand, load2_demand, gen_costs, noload_costs, model=model, silent=silent)\n    function pullback_unit_commitment(pb)\n        p = model[:p]\n        u = model[:u]\n        energy_balance_cons = model[:energy_balance_cons]\n\n        MOI.set.(model, DiffOpt.BackwardInVariablePrimal(), p, pb)\n        DiffOpt.backward(JuMP.backend(model))\n\n        obj = MOI.get(model, DiffOpt.BackwardOutObjective())\n\n        # computing derivative wrt linear objective costs\n        dgen_costs = similar(gen_costs)\n        dgen_costs[1] = sum(JuMP.coefficient.(obj, p[1,:]))\n        dgen_costs[2] = sum(JuMP.coefficient.(obj, p[2,:]))\n\n        dnoload_costs = similar(noload_costs)\n        dnoload_costs[1] = sum(JuMP.coefficient.(obj, u[1,:]))\n        dnoload_costs[2] = sum(JuMP.coefficient.(obj, u[2,:]))\n\n        # computing derivative wrt constraint constant\n        dload1_demand = JuMP.constant.(MOI.get.(model, DiffOpt.BackwardOutConstraint(), energy_balance_cons))\n        dload2_demand = copy(dload1_demand)\n        return (dload1_demand, dload2_demand, dgen_costs, dnoload_costs)\n    end\n    return (pv, pullback_unit_commitment)\nend","category":"page"},{"location":"chainrules_unit/","page":"ChainRules integration","title":"ChainRules integration","text":"We can set a seed of one on the power of the first generator at the second time frame and zero for all other parts of the solution:","category":"page"},{"location":"chainrules_unit/","page":"ChainRules integration","title":"ChainRules integration","text":"(pv, pullback_unit_commitment) = ChainRulesCore.rrule(\n    unit_commitment,\n    load1_demand, load2_demand, gen_costs, noload_costs,\n    optimizer=Clp.Optimizer,\n    silent=true,\n)\ndpv = 0 * pv\ndpv[1,2] = 1\ndargs = pullback_unit_commitment(dpv)\n(dload1_demand, dload2_demand, dgen_costs, dnoload_costs) = dargs\nnothing # hide","category":"page"},{"location":"chainrules_unit/","page":"ChainRules integration","title":"ChainRules integration","text":"The sensitivities with respect to the load demands are:","category":"page"},{"location":"chainrules_unit/","page":"ChainRules integration","title":"ChainRules integration","text":"dload1_demand","category":"page"},{"location":"chainrules_unit/","page":"ChainRules integration","title":"ChainRules integration","text":"dload2_demand","category":"page"},{"location":"chainrules_unit/","page":"ChainRules integration","title":"ChainRules integration","text":"The sensitivity of the generation is propagated to the sensitivity of both loads at the second time frame.","category":"page"},{"location":"chainrules_unit/","page":"ChainRules integration","title":"ChainRules integration","text":"This example integrating ChainRules was designed with support from Invenia Technical Computing.","category":"page"},{"location":"usage/#Usage","page":"Usage","title":"Usage","text":"","category":"section"},{"location":"usage/","page":"Usage","title":"Usage","text":"Create a differentiable model from existing optimizers","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"    using DiffOpt\n    using SCS\n    \n    model = diff_optimizer(SCS.Optimizer)","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Update and solve the model ","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"    x = MOI.add_variables(model, 2)\n    c = MOI.add_constraint(model, ...)\n    \n    MOI.optimize!(model)","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Finally differentiate the model (primal and dual variables specifically) to obtain product of jacobians with respect to problem parameters and a backward pass vector. Currently DiffOpt supports two backends for differentiating a model:","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"To differentiate Convex Quadratic Program","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"beginalign*\n min_x in mathbbR^n  frac12 x^T Q x + q^T x   \n textst                A x = b        qquad         b in mathbbR^m \n                            G x leq h     qquad         h in mathbbR^p\nendalign*","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"we can use the backward method","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"MOI.set.(model,\n    DiffOpt.BackwardInVariablePrimal(), x, ones(2))\nDiffOpt.backward(model)\ngrad_obj = MOI.get(model, DiffOpt.BackwardOutObjective())\ngrad_con = MOI.get.(model, DiffOpt.BackwardOutConstraint(), c)","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"To differentiate convex conic program","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"beginalign*\n min_x in mathbbR^n  c^T x \n textst                A x + s = b  \n                            b in mathbbR^m  \n                            s in mathcalK\nendalign*","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"we can use the forward method with perturbations in matrices A, b, c","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"using LinearAlgebra # for `⋅`\nMOI.set(model, DiffOpt.ForwardInObjective(), ones(2) ⋅ MOI.SingleVariable.(x))\nDiffOpt.forward(model)\ngrad_x = MOI.get.(model, DiffOpt.ForwardOutVariablePrimal(), x)","category":"page"},{"location":"matrix-inversion-manual/#Differentiating-a-QP-wrt-a-single-variable","page":"Differentiating a simple QP by hand","title":"Differentiating a QP wrt a single variable","text":"","category":"section"},{"location":"matrix-inversion-manual/","page":"Differentiating a simple QP by hand","title":"Differentiating a simple QP by hand","text":"Consider the quadratic program","category":"page"},{"location":"matrix-inversion-manual/","page":"Differentiating a simple QP by hand","title":"Differentiating a simple QP by hand","text":"beginsplit\nbeginarray ll\nmboxminimize  frac12 x^T Q x + q^T x \nmboxsubject to  G x leq h x in mathcalR^2 h in mathcalR \nendarray\nendsplit","category":"page"},{"location":"matrix-inversion-manual/","page":"Differentiating a simple QP by hand","title":"Differentiating a simple QP by hand","text":"where Q, q, G are fixed and h is the single parameter.","category":"page"},{"location":"matrix-inversion-manual/","page":"Differentiating a simple QP by hand","title":"Differentiating a simple QP by hand","text":"In this example, we'll try to differentiate the QP wrt h, by finding its jacobian by hand (using Eqn (6) of QPTH article) and compare the results:","category":"page"},{"location":"matrix-inversion-manual/","page":"Differentiating a simple QP by hand","title":"Differentiating a simple QP by hand","text":"In python, using CVXPYLayers - https://github.com/cvxgrp/cvxpylayers#tensorflow-2\nIn Julia, using LinearAlgebra, Dualization.jl and MOI","category":"page"},{"location":"matrix-inversion-manual/","page":"Differentiating a simple QP by hand","title":"Differentiating a simple QP by hand","text":"Assuming ","category":"page"},{"location":"matrix-inversion-manual/","page":"Differentiating a simple QP by hand","title":"Differentiating a simple QP by hand","text":"Q = [[4, 1], [1, 2]]\nq = [1, 1]\nG = [1, 1]","category":"page"},{"location":"matrix-inversion-manual/","page":"Differentiating a simple QP by hand","title":"Differentiating a simple QP by hand","text":"and begining with a starting value of h=-1","category":"page"},{"location":"matrix-inversion-manual/","page":"Differentiating a simple QP by hand","title":"Differentiating a simple QP by hand","text":"few values just for reference","category":"page"},{"location":"matrix-inversion-manual/","page":"Differentiating a simple QP by hand","title":"Differentiating a simple QP by hand","text":"variable optimal value note\nx* [-0.25; -0.75] Primal optimal\n𝜆∗ -0.75 Dual optimal","category":"page"},{"location":"matrix-inversion-manual/#Finding-Jacobian-using-matrix-inversion","page":"Differentiating a simple QP by hand","title":"Finding Jacobian using matrix inversion","text":"","category":"section"},{"location":"matrix-inversion-manual/","page":"Differentiating a simple QP by hand","title":"Differentiating a simple QP by hand","text":"Lets formulate Eqn (6) of QPTH article for our QP. If we assume h as the only parameter and Q,q,G as fixed problem data - also note that our QP doesn't involves Ax=b constraint - then Eqn (6) reduces to ","category":"page"},{"location":"matrix-inversion-manual/","page":"Differentiating a simple QP by hand","title":"Differentiating a simple QP by hand","text":"begingather\n beginbmatrix \n     Q  g^T \n     lambda^* g  g z^* - h\n endbmatrix\n beginbmatrix \n     dz \n     d lambda\n endbmatrix\n =\n  beginbmatrix\n   0 \n   lambda^* dh\n   endbmatrix\nendgather","category":"page"},{"location":"matrix-inversion-manual/","page":"Differentiating a simple QP by hand","title":"Differentiating a simple QP by hand","text":"Now to find the jacobians $ \\frac{\\partial z}{\\partial h}, \\frac{\\partial \\lambda}{\\partial h}$ we substitute dh = I = [1] and plug in values of Q,q,G to get","category":"page"},{"location":"matrix-inversion-manual/","page":"Differentiating a simple QP by hand","title":"Differentiating a simple QP by hand","text":"begingather\n beginbmatrix \n     4  1  1 \n     1  2  1 \n     -075  -075  0\n endbmatrix\n beginbmatrix \n     fracpartial z_1partial h \n     fracpartial z_2partial h \n     fracpartial lambdapartial h\n endbmatrix\n =\n  beginbmatrix\n   0 \n   0 \n   -075\n   endbmatrix\nendgather","category":"page"},{"location":"matrix-inversion-manual/","page":"Differentiating a simple QP by hand","title":"Differentiating a simple QP by hand","text":"Upon solving using matrix inversion, the jacobian is","category":"page"},{"location":"matrix-inversion-manual/","page":"Differentiating a simple QP by hand","title":"Differentiating a simple QP by hand","text":"fracpartial z_1partial h = 025 fracpartial z_2partial h = 075 fracpartial lambdapartial h = -175 ","category":"page"},{"location":"matrix-inversion-manual/#Finding-Jacobian-in-CVXPYLayers","page":"Differentiating a simple QP by hand","title":"Finding Jacobian in CVXPYLayers","text":"","category":"section"},{"location":"matrix-inversion-manual/","page":"Differentiating a simple QP by hand","title":"Differentiating a simple QP by hand","text":"import cvxpy as cp\nimport tensorflow as tf\nfrom cvxpylayers.tensorflow import CvxpyLayer\n\nn, m = 2, 1\nx = cp.Variable(n)\nQ = np.array([[4, 1], [1, 2]])\nq = np.array([1, 1])\nG = np.array([1, 1])\nh = cp.Parameter(m)\nconstraints = [G@x <= h]\nobjective = cp.Minimize(0.5*cp.quad_form(x, Q) + q.T @ x)\nproblem = cp.Problem(objective, constraints)\nassert problem.is_dpp()\n\ncvxpylayer = CvxpyLayer(problem, parameters=[h], variables=[x])\nh_tf = tf.Variable([-1.0])  # set a starting value\n\nwith tf.GradientTape() as tape:\n  # solve the problem, setting the values of h to h_tf\n  solution, = cvxpylayer(h_tf)\n\n  summed_solution = tf.math.reduce_sum(solution)\n  \n# note - solution is [-0.25, -0.75]\n#        summed_solution is (-0.25) + (-0.75)\n\n# cvxpylayers allows gradient of the summed solution only, with respect to h\ngradh = tape.gradient(summed_solution, [h_tf])","category":"page"},{"location":"matrix-inversion-manual/#Finding-Jacobian-using-MOI,-Dualization.jl,-LinearAlgebra.jl","page":"Differentiating a simple QP by hand","title":"Finding Jacobian using MOI, Dualization.jl, LinearAlgebra.jl","text":"","category":"section"},{"location":"matrix-inversion-manual/","page":"Differentiating a simple QP by hand","title":"Differentiating a simple QP by hand","text":"using Random\nusing MathOptInterface\nusing Dualization\nusing OSQP\nusing LinearAlgebra\n\nconst MOI = MathOptInterface\nconst MOIU = MathOptInterface.Utilities;\n\nn = 2 # variable dimension\nm = 1; # no of inequality constraints\n\nQ = [4. 1.;1. 2.]\nq = [1.; 1.]\nG = [1. 1.;]\nh = [-1.;]   # initial values set\n\n\n# create the optimizer\nmodel = MOI.instantiate(OSQP.Optimizer, with_bridge_type=Float64)\nx = MOI.add_variables(model, n);\n\n# define objective\nquad_terms = MOI.ScalarQuadraticTerm{Float64}[]\nfor i in 1:n\n    for j in i:n # indexes (i,j), (j,i) will be mirrored. specify only one kind\n        push!(quad_terms, MOI.ScalarQuadraticTerm(Q[i,j],x[i],x[j]))\n    end\nend\n\nobjective_function = MOI.ScalarQuadraticFunction(MOI.ScalarAffineTerm.(q, x),quad_terms,0.)\nMOI.set(model, MOI.ObjectiveFunction{MOI.ScalarQuadraticFunction{Float64}}(), objective_function)\nMOI.set(model, MOI.ObjectiveSense(), MOI.MIN_SENSE)\n\n# add constraint\nMOI.add_constraint(\n    model,\n    MOI.ScalarAffineFunction(MOI.ScalarAffineTerm.(G[1,:], x), 0.),\n    MOI.LessThan(h[1])\n)\n\n# solve\nMOI.optimize!(model)\n\n# sanity-check\n@assert MOI.get(model, MOI.TerminationStatus()) in [MOI.LOCALLY_SOLVED, MOI.OPTIMAL]\n\nx̄ = MOI.get(model, MOI.VariablePrimal(), x)\n\n# obtaining λ*\n\njoint_object    = dualize(model)\ndual_model_like = joint_object.dual_model # this is MOI.ModelLike, not an MOI.AbstractOptimizer; can't call optimizer on it\nprimal_dual_map = joint_object.primal_dual_map;\n\n# copy the dual model objective, constraints, and variables to an optimizer\ndual_model = MOI.instantiate(OSQP.Optimizer, with_bridge_type=Float64)\nMOI.copy_to(dual_model, dual_model_like)\n\n# solve dual\nMOI.optimize!(dual_model);\n\nmap = primal_dual_map.primal_con_dual_var\n\nfor con_index in keys(map)\n    λ = MOI.get(dual_model, MOI.VariablePrimal(), map[con_index][1])\n    println(λ)\nend\n\nLHS = [4 1 1; 1 2 1; 1 1 0]  # of Eqn (6)\nRHS = [0; 0; 1]  # of Eqn (6)\n\npp \\ qq  # the jacobian","category":"page"},{"location":"matrix-inversion-manual/","page":"Differentiating a simple QP by hand","title":"Differentiating a simple QP by hand","text":"3-element Array{Float64,1}:\n  0.25\n  0.75\n -1.75","category":"page"},{"location":"#DiffOpt.jl","page":"Home","title":"DiffOpt.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"DiffOpt.jl is a package for differentiating convex optimization program (JuMP.jl or MathOptInterface.jl models) with respect to program parameters. Note that this package does not contain any solver. This package has two major backends, available via backward and forward methods, to differentiate models (quadratic or conic) with optimal solutions.","category":"page"},{"location":"","page":"Home","title":"Home","text":"note: Note\nCurrently supports linear programs (LP), convex quadratic programs (QP) and convex conic programs (SDP, SOCP constraints only). ","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"DiffOpt can be installed through the Julia package manager:","category":"page"},{"location":"","page":"Home","title":"Home","text":"(v1.3) pkg> add https://github.com/jump-dev/DiffOpt.jl","category":"page"},{"location":"#Why-are-Differentiable-optimization-problems-important?","page":"Home","title":"Why are Differentiable optimization problems important?","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Differentiable optimization is a promising field of convex optimization and has many potential applications in game theory, control theory and machine learning (specifically deep learning - refer this video for more). Recent work has shown how to differentiate specific subclasses of convex optimization problems. But several applications remain unexplored (refer section 8 of this really good thesis). With the help of automatic differentiation, differentiable optimization can have a significant impact on creating end-to-end differentiable systems to model neural networks, stochastic processes, or a game.","category":"page"},{"location":"#Contributing","page":"Home","title":"Contributing","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Contributions to this package are more than welcome, if you find a bug or have any suggestions for the documentation please post it on the github issue tracker.","category":"page"},{"location":"","page":"Home","title":"Home","text":"When contributing please note that the package follows the JuMP style guide","category":"page"}]
}
